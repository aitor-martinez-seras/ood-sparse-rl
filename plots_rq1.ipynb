{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import List\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as sk_metrics\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 for OOD, 0 for IND\n",
    "# Now, for each TPR threshold, we compute the TP, FP, TN, FN values\n",
    "# We also compute the precision, recall (TPR) and FPR \n",
    "# True Positive     (TP) is an IND detected as IND\n",
    "# False Positive    (FP) is an OOD detected as IND\n",
    "# True Negative     (TN) is an OOD detected as OOD\n",
    "# False Negative    (FN) is an IND detected as OOD\n",
    "# TPR = TP / (TP + FN)\n",
    "# FPR = FP / (FP + TN)\n",
    "# precision = TP / (TP + FP)\n",
    "def compute_tpr_fpr_precision_for_auroc(activations: np.array, ground_truth: np.array, thresholds: np.array) -> (np.array, np.array, np.array):\n",
    "    TPR = []\n",
    "    FPR = []\n",
    "    PRECISION = []\n",
    "\n",
    "    # Compute the predictions for each TPR threshold\n",
    "    preds_per_thr = []\n",
    "    for thr in thresholds:\n",
    "        preds_per_thr.append((activations >= thr).astype(int))\n",
    "    \n",
    "    # Compute the TP, FP, TN, FN values for the predictions, all stored in one array\n",
    "    tp_fp_tn_fn = []\n",
    "    tpr_fpr_precision = []\n",
    "    for i, preds_one_thr in enumerate(preds_per_thr):\n",
    "        tp = np.sum(preds_one_thr[ground_truth == 0] == 0)\n",
    "        fp = np.sum(preds_one_thr[ground_truth == 1] == 0)\n",
    "        tn = np.sum(preds_one_thr[ground_truth == 1] == 1)\n",
    "        fn = np.sum(preds_one_thr[ground_truth == 0] == 1)\n",
    "        tp_fp_tn_fn.append([tp, fp, tn, fn])\n",
    "        tpr = tp / (tp + fn)\n",
    "        fpr = fp / (fp + tn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        tpr_fpr_precision.append([tpr, fpr, precision])\n",
    "        # TPR.append(tpr)\n",
    "        # FPR.append(fpr)\n",
    "        # PRECISION.append(precision)\n",
    "\n",
    "    # TPR = np.array(TPR)\n",
    "    # FPR = np.array(FPR)\n",
    "    # PRECISION = np.array(PRECISION)\n",
    "    \n",
    "    #tpr_fpr_precision = np.sort(np.array(tpr_fpr_precision), axis=0)\n",
    "    tpr_fpr_precision = np.sort(np.array(tpr_fpr_precision), axis=0)\n",
    "    \n",
    "    return tpr_fpr_precision\n",
    "\n",
    "\n",
    "def plot_auroc(tpr_values, fpr_values):\n",
    "    auroc_value = np.abs(np.trapz(tpr_values, fpr_values))\n",
    "    tpr_percentages = np.arange(0, 1.01, 0.01)\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    plt.plot(fpr_values, tpr_values, label='ROC curve', lw=3)\n",
    "    plt.plot(tpr_percentages, tpr_percentages, 'k--', label='Random ROC curve')\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('FPR', fontsize=20)\n",
    "    plt.ylabel('TPR', fontsize=20)\n",
    "    plt.title(f'ROC curve, AUC = {auroc_value:0.3f}', fontsize=25, pad=10)\n",
    "    plt.fill_between(fpr_values, tpr_values, alpha=0.3)\n",
    "    # plt.plot([], [], ' ', label=f'FPR at 95% TPR = {fpr_values[95] * 100:.2f}%')\n",
    "    # plt.plot([], [], ' ', label=f'FPR at 80% TPR = {fpr_values[80] * 100:.2f}%')\n",
    "    # plt.text(0.60,0.975,'FPR at 95% TPR = {}%'.format(round(array_TPR_FPR_x_threshold[95,1]*100,2)),fontsize=20,bbox=dict(boxstyle=\"round\",facecolor='white', alpha=0.5))\n",
    "    plt.legend(fontsize=20, loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_auroc_per_method(results_per_method: dict, per_step_results: bool = False, which_tpr_for_the_fpr=0) -> dict:\n",
    "\n",
    "    # Compute AUROC and sort the results by the AUROC value\n",
    "    auroc_values_per_method = OrderedDict()\n",
    "    methods_names = []\n",
    "    fpr_at_certain_tpr = []\n",
    "    for method_name, results in results_per_method.items():\n",
    "        if isinstance(results, list):\n",
    "            TPR, FPR, PRECISION = results\n",
    "\n",
    "        elif isinstance(results, np.ndarray):\n",
    "            TPR, FPR, PRECISION = results[:, 0], results[:, 1], results[:, 2]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"The results must be a tuple, list or array\")\n",
    "\n",
    "        if which_tpr_for_the_fpr > 0:\n",
    "            # Compute the TPR at a certain FPR\n",
    "            index_closest = np.argmin(np.abs(TPR - which_tpr_for_the_fpr))\n",
    "            fpr_at_certain_tpr.append(FPR[index_closest])\n",
    "            auroc_values_per_method[method_name] = [np.abs(np.trapz(TPR, FPR)), FPR[index_closest]]\n",
    "        else:\n",
    "            auroc_values_per_method[method_name] = np.abs(np.trapz(TPR, FPR))\n",
    "\n",
    "    # # Convert the lists into numpy arrays\n",
    "    # auroc_values = np.array(auroc_values)\n",
    "\n",
    "    return auroc_values_per_method\n",
    "\n",
    "def plot_auroc_per_method(results_per_method: dict, figsize, title, which_tpr_to_plot: float = 0.80, plot_random_curve: bool = False, save_path: Path = None):\n",
    "    \n",
    "    # Create the figure\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Fontsize as a function of the figure size\n",
    "    fontsize = int(figsize[0] * 2)\n",
    "\n",
    "    # # Only considering the first column\n",
    "    # first_column = results_per_method['forward_dynamics_l1'][:, 0]\n",
    "\n",
    "    # # Find the index of the value closest to the target\n",
    "    # index_closest = np.argmin(np.abs(first_column - which_fpr_to_plot))\n",
    "\n",
    "    # Compute AUROC and sort the results by the AUROC value\n",
    "    auroc_values = []\n",
    "    methods_names = []\n",
    "    fpr_at_certain_tpr = []\n",
    "    for method_name, results in results_per_method.items():\n",
    "        if isinstance(results, list):\n",
    "            TPR, FPR, PRECISION = results\n",
    "\n",
    "        elif isinstance(results, np.ndarray):\n",
    "            TPR, FPR, PRECISION = results[:, 0], results[:, 1], results[:, 2]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"The results must be a tuple, list or array\")\n",
    "        \n",
    "        # Add the method name to the list\n",
    "        methods_names.append(method_name)\n",
    "        \n",
    "        # Compute the AUROC value\n",
    "        auroc_values.append(np.abs(np.trapz(TPR, FPR)))\n",
    "        \n",
    "        # Compute the TPR at a certain FPR\n",
    "        index_closest = np.argmin(np.abs(TPR - which_tpr_to_plot))\n",
    "        fpr_at_certain_tpr.append(FPR[index_closest])      \n",
    "\n",
    "    # Convert the lists into numpy arrays\n",
    "    auroc_values = np.array(auroc_values)\n",
    "    \n",
    "    # Sort the methods by the AUROC value\n",
    "    sorted_indices = np.argsort(auroc_values)[::-1]\n",
    "    auroc_values = auroc_values[sorted_indices]\n",
    "    methods_names = np.array(methods_names)[sorted_indices]\n",
    "    fpr_at_certain_tpr = np.array(fpr_at_certain_tpr)[sorted_indices]\n",
    "\n",
    "    # Plot the AUROC values\n",
    "    for method_name, auroc_value in zip(methods_names, auroc_values):\n",
    "        TPR, FPR, PRECISION = results_per_method[method_name][:, 0], results_per_method[method_name][:, 1], results_per_method[method_name][:, 2]\n",
    "        plt.plot(FPR, TPR, label=rf'AUC={auroc_value:0.3f} - {method_name} ', lw=3)\n",
    "\n",
    "    # Random curve\n",
    "    if plot_random_curve:\n",
    "        random_classifier_points = np.arange(0, 1.01, 0.01)\n",
    "        plt.plot(random_classifier_points, random_classifier_points, 'k--', label='Random')\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.xlabel('FPR', fontsize=fontsize)\n",
    "    plt.ylabel('TPR', fontsize=fontsize)\n",
    "    plt.title(f'ROC curve- {title}', fontsize=fontsize, pad=10)\n",
    "    plt.legend(fontsize=fontsize, loc='lower right')\n",
    "\n",
    "    if which_tpr_to_plot > 0:\n",
    "        # Plot the TPR at a certain FPR as text. We will plot the text in the same order as the methods are plotted\n",
    "        # To do so, we will iterate over the sorted indices to accumulate the text as a big string containing the substring of \n",
    "        # each methods TPR value\n",
    "        fpr_at_certain_tpr_text = \"\"\n",
    "        # Put a title to the text\n",
    "        fpr_at_certain_tpr_text += f\"FPR at {which_tpr_to_plot * 100:.0f}% TPR\\n\"\n",
    "        for method_name, fpr_at_certain_tpr_value in zip(methods_names, fpr_at_certain_tpr):\n",
    "            fpr_at_certain_tpr_text += f\" {fpr_at_certain_tpr_value * 100:.2f}% - {method_name}\\n\"\n",
    "        plt.text(0.50,0.5,fpr_at_certain_tpr_text,fontsize=20,bbox=dict(boxstyle=\"round\",facecolor='white', alpha=0.5))\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "# plt.text(0.60,0.975,'FPR at 95% TPR = {}%'.format(round(array_TPR_FPR_x_threshold[95,1]*100,2)),fontsize=20,bbox=dict(boxstyle=\"round\",facecolor='white', alpha=0.5))\n",
    "\n",
    "def extract_tpr_fpr_precision_from_results_file(results: dict, per_step_results: bool) -> np.ndarray:\n",
    "    # Extract the IND activations for the thresholds and all the activations for the tests\n",
    "    if per_step_results:\n",
    "        all_activations = results[:, 2]\n",
    "        ground_truth_per_step = results[:, 0]\n",
    "    else:\n",
    "        all_activations = []\n",
    "        ground_truth_per_step = []\n",
    "        for k, v in results.items():\n",
    "            all_activations.append(v[:, 1])\n",
    "            ground_truth_per_step.append(v[:, 0])\n",
    "        # Concatenate all the activations and ground truth\n",
    "        all_activations = np.concatenate(all_activations)\n",
    "        ground_truth_per_step = np.concatenate(ground_truth_per_step)\n",
    "    # Sort the activations and ground truth\n",
    "    sorted_indices = np.argsort(all_activations)\n",
    "    all_activations = all_activations[sorted_indices]\n",
    "    ground_truth_per_step = ground_truth_per_step[sorted_indices]\n",
    "    # Create the in and out of distribution activations arrays\n",
    "    in_distribution_activations = all_activations[ground_truth_per_step == 0]\n",
    "    ood_activations = all_activations[ground_truth_per_step == 1]\n",
    "\n",
    "    # Compute the thresholds to cover all the auroc curve (from 0 to 1 tpr and fpr)\n",
    "    tpr_fpr_percentages = np.arange(0, 1.01, 0.01)\n",
    "    thresholds = []\n",
    "    for percent in tpr_fpr_percentages:\n",
    "        thresholds.append(np.quantile(in_distribution_activations, percent, method=\"nearest\"))\n",
    "        thresholds.append(np.quantile(ood_activations, percent, method=\"nearest\"))\n",
    "    thresholds = np.unique(thresholds)\n",
    "\n",
    "    # Compute the TPR, FPR and precision for each threshold\n",
    "    tpr_fpr_precision = compute_tpr_fpr_precision_for_auroc(all_activations, ground_truth_per_step, thresholds)\n",
    "\n",
    "    return tpr_fpr_precision\n",
    "\n",
    "\n",
    "def extract_tpr_fpr_precision_per_distance_to_ood_from_results_file(results: dict, per_step_results: bool) -> List:\n",
    "    all_activations = results[:, 2]\n",
    "    ground_truth_distance_per_step = results[:, 1]\n",
    "    # Sort the activations and ground truth\n",
    "    sorted_indices = np.argsort(all_activations)\n",
    "    all_activations = all_activations[sorted_indices]\n",
    "    ground_truth_distance_per_step = ground_truth_distance_per_step[sorted_indices]\n",
    "\n",
    "    # Now we evaluate for each distance to the OOD element separately\n",
    "    idx_in_distribution = np.argwhere(ground_truth_distance_per_step == 0)[:,0]\n",
    "    in_distribution_activations = all_activations[idx_in_distribution]\n",
    "    tpr_fpr_precision_per_distance = []\n",
    "    for distance in np.unique(ground_truth_distance_per_step)[1:]:  # We skip the 0 distance\n",
    "        # Create the in and out of distribution activations arrays\n",
    "        idx_oods = np.argwhere(ground_truth_distance_per_step == distance)[:,0]\n",
    "        ood_activations = all_activations[idx_oods]\n",
    "        # Compute the thresholds to cover all the auroc curve (from 0 to 1 tpr and fpr)\n",
    "        tpr_fpr_percentages = np.arange(0, 1.01, 0.01)\n",
    "        thresholds = []\n",
    "        for percent in tpr_fpr_percentages:\n",
    "            thresholds.append(np.quantile(in_distribution_activations, percent, method=\"nearest\"))\n",
    "            thresholds.append(np.quantile(ood_activations, percent, method=\"nearest\"))\n",
    "        thresholds = np.unique(thresholds)\n",
    "\n",
    "        # Compute the TPR, FPR and precision for each threshold. The ground truth is the distance to the OOD element\n",
    "        # so we must convert it to a binary ground truth (0 for IND, 1 for OOD)\n",
    "        ground_truth_for_one_distance = (np.zeros_like(ground_truth_distance_per_step)+1)*-1\n",
    "        ground_truth_for_one_distance[idx_oods] = 1\n",
    "        ground_truth_for_one_distance[idx_in_distribution] = 0\n",
    "\n",
    "        tpr_fpr_precision_per_distance.append([distance, compute_tpr_fpr_precision_for_auroc(all_activations, ground_truth_for_one_distance, thresholds)])\n",
    "    return tpr_fpr_precision_per_distance\n",
    "\n",
    "def compute_auroc_and_fpr_for_all_seeds(results_per_seed: List, which_tpr_for_the_fpr=0.95) -> dict:\n",
    "    fpr_at_certain_tpr = []\n",
    "    auroc_fpr_all_seeds = []\n",
    "    for i, values in enumerate(results_per_seed):\n",
    "        \n",
    "        TPR, FPR, PRECISION = values[:, 0], values[:, 1], values[:, 2]\n",
    "\n",
    "        # Compute the TPR at a certain FPR\n",
    "        index_closest = np.argmin(np.abs(TPR - which_tpr_for_the_fpr))\n",
    "        fpr_at_certain_tpr.append(FPR[index_closest])\n",
    "        auroc_fpr_all_seeds.append([np.abs(np.trapz(TPR, FPR)), FPR[index_closest]])\n",
    "\n",
    "    # Convert the lists into numpy arrays\n",
    "    auroc_fpr_all_seeds = np.array(auroc_fpr_all_seeds)\n",
    "    # Now compute the mean and std\n",
    "    auroc_mean = np.mean(auroc_fpr_all_seeds[:, 0])\n",
    "    auroc_std = np.std(auroc_fpr_all_seeds[:, 0])\n",
    "    fpr_mean = np.mean(auroc_fpr_all_seeds[:, 1])\n",
    "    fpr_std = np.std(auroc_fpr_all_seeds[:, 1])\n",
    "    \n",
    "    # Create an array with the mean and std of the auroc and fpr\n",
    "    auroc_fpr_mean_std = np.array([[auroc_mean, auroc_std], [fpr_mean, fpr_std]])\n",
    "    return auroc_fpr_mean_std\n",
    "\n",
    "#def compute_auroc_and_fpr_vs_distance_for_all_seeds()\n",
    "    \n",
    "def avg_and_std_per_distance(auroc_per_seed_per_distance: List) -> List:\n",
    "    auroc_per_distance = []\n",
    "\n",
    "    #print(auroc_per_seed_per_distance)\n",
    "    # print(len(auroc_per_seed_per_distance))\n",
    "    # print(len(auroc_per_seed_per_distance[0]))\n",
    "    # Accumulate the auroc of each seed for each distance\n",
    "    auroc_per_distance_all_seeds = OrderedDict()\n",
    "    for i in range(1, len(auroc_per_seed_per_distance[0])+1):\n",
    "        auroc_per_distance_all_seeds[i] = []\n",
    "\n",
    "    for i, values_one_seed in enumerate(auroc_per_seed_per_distance):\n",
    "        for distance, auroc_val, fpr_at_certain_tpr in values_one_seed:\n",
    "            auroc_per_distance_all_seeds[int(distance)].append(auroc_val)\n",
    "    \n",
    "    # Now compute the mean and std\n",
    "    for d, auroc_one_dist_all_seeds in auroc_per_distance_all_seeds.items():\n",
    "        #print(d, auroc_one_dist_all_seeds)\n",
    "        auroc_per_distance.append([np.mean(auroc_one_dist_all_seeds), np.std(auroc_one_dist_all_seeds)])\n",
    "    \n",
    "    return auroc_per_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data from seeds (AUROC and per env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER STEP and for all environments\n",
    "# Define folder path\n",
    "detection_results_folder = Path(\"ood_storage/detection_results\")\n",
    "\n",
    "ood_settings_to_benchmark = [\n",
    "    'Lava010',\n",
    "    'Ball100',\n",
    "    'Sand050',\n",
    "]\n",
    "\n",
    "per_step_or_per_level_evaluation = 'per_steps'\n",
    "\n",
    "seeds = [1,2,3,4,5]\n",
    "\n",
    "model_name = 'P_MN5S8_1000_lvls_r_ep_im005_ent00005'\n",
    "\n",
    "methods_to_benchmark = ['forward_dynamics_l1', 'forward_dynamics_l2', 'l1', 'l2',\n",
    "                        'l1_per_action', 'l2_per_action', 'msp', 'energy',]\n",
    "\n",
    "# Useless parameter, but it is required to load the file. Leave it as 9\n",
    "ood_detection_distance = 9\n",
    "\n",
    "# Only used in case we want to compute the TPR at a certain FPR\n",
    "which_tpr_for_the_fpr = 0.95\n",
    "\n",
    "results_per_ood_env_per_method = OrderedDict()\n",
    "auroc_per_env_per_method_per_distance = OrderedDict()\n",
    "for ood_env in ood_settings_to_benchmark:\n",
    "    \n",
    "    # Only consider the .pt files in the folder\n",
    "    results_per_method = OrderedDict()\n",
    "    # results_per_method_per_distance = OrderedDict()\n",
    "    auroc_per_method_per_distance = OrderedDict()\n",
    "    for method_name in methods_to_benchmark:\n",
    "        \n",
    "        auroc_per_seed_per_distance = []\n",
    "        results_per_seed = []\n",
    "        for s in seeds:\n",
    "            \n",
    "            file_name = f'ood_detection_results_per_steps_{method_name}_{model_name}_{s}_{ood_env}_ood_performance_evaluation_normal_vs_lava_09_{s:02d}.pt'\n",
    "\n",
    "            file_path = detection_results_folder / file_name\n",
    "\n",
    "            # if not file_path.exists():\n",
    "            #     print(f\"File {file_path} does not exist\")\n",
    "            #     continue\n",
    "            # continue\n",
    "\n",
    "            # Load the results\n",
    "            results = torch.load(file_path)\n",
    "\n",
    "            ## Results for all distances ##\n",
    "            # Results for all distances. Compute the TPR, FPR and precision to plot auroc. Store the results\n",
    "            results_per_seed.append(extract_tpr_fpr_precision_from_results_file(results=results, per_step_results=True))\n",
    "\n",
    "            ## Results per distance to the OOD element ##\n",
    "            # Each position in this list is a distance to the OOD element. Each distance has the TPR, FPR and precision for each threshold\n",
    "            tpr_fpr_precision_per_distance_one_seed = extract_tpr_fpr_precision_per_distance_to_ood_from_results_file(results, per_step_results=True)\n",
    "\n",
    "            # Compute the AUROC per distance for each seed. Each position in the list is a seed, and each seed has a list of distances (1 to 9)\n",
    "            #total_auroc_per_method[method_name] = np.abs(np.trapz(results_per_method[method_name][:, 0], results_per_method[method_name][:, 1]))\n",
    "            #auroc_and_fpr_values_per_method_per_distance[method_name] = []\n",
    "            values_per_distance = []\n",
    "            for dist in tpr_fpr_precision_per_distance_one_seed:\n",
    "                TPR, FPR, PRECISION = dist[1][:, 0], dist[1][:, 1], dist[1][:, 2]\n",
    "                auroc_value = np.abs(np.trapz(TPR, FPR))\n",
    "                # Compute the TPR at a certain FPR\n",
    "                index_closest = np.argmin(np.abs(TPR - which_tpr_for_the_fpr))\n",
    "                fpr_at_certain_tpr = FPR[index_closest]\n",
    "                values_per_distance.append([dist[0], auroc_value, fpr_at_certain_tpr])\n",
    "            # Store the results for each seed\n",
    "            auroc_per_seed_per_distance.append(values_per_distance)            \n",
    "\n",
    "            ### Finished one seed ###\n",
    "        \n",
    "        ## All distances ##\n",
    "        # Auroc for all distances, one method, for all seeds. Store the results of each method\n",
    "        results_per_method[method_name] = compute_auroc_and_fpr_for_all_seeds(results_per_seed=results_per_seed, which_tpr_for_the_fpr=0.95)\n",
    "        # The array has the shape (2, 2), where the first row is the mean and std of the auroc and the second row is the mean and std of the fpr\n",
    "        #  [[auroc_mean, auroc_std],\n",
    "        #   [fpr_mean,   fpr_std  ]]         \n",
    "\n",
    "        ## Per distance ##\n",
    "        # Auroc per distance, one method, for all seeds\n",
    "        auroc_per_method_per_distance[method_name] = avg_and_std_per_distance(auroc_per_seed_per_distance)\n",
    "\n",
    "        print(f\"Finished method {method_name} for env {ood_env}\")\n",
    "    \n",
    "    # Store the results of each environment\n",
    "    # All distances\n",
    "    results_per_ood_env_per_method[ood_env] = results_per_method\n",
    "\n",
    "    # Per distance\n",
    "    auroc_per_env_per_method_per_distance[ood_env] = auroc_per_method_per_distance\n",
    "\n",
    "# Convert the lists into numpy arrays\n",
    "for env in auroc_per_env_per_method_per_distance.keys():\n",
    "    for method in auroc_per_env_per_method_per_distance[env].keys():\n",
    "        #print(f\"{env} - {method}: {np.array(auroc_per_env_per_method_per_distance[env][method]).shape}\")\n",
    "        auroc_per_env_per_method_per_distance[env][method] = np.array(auroc_per_env_per_method_per_distance[env][method])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of AUROC per method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results auroc_per_env_per_method_per_distance as a pickle file\n",
    "with open(f'ood_storage/auroc_per_env_per_method_per_distance.pkl', 'wb') as f:\n",
    "    pickle.dump(auroc_per_env_per_method_per_distance, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming into a Pandas DataFrame\n",
    "# Flattening the data and keeping only the first value of each list\n",
    "flattened_data = {scenario: {method: values[0][0] for method, values in methods.items()} \n",
    "                  for scenario, methods in results_per_ood_env_per_method.items()}\n",
    "\n",
    "df = pd.DataFrame(flattened_data)\n",
    "# Add a new column with the average\n",
    "#df['Average'] = df.mean(axis=1)\n",
    "\n",
    "# Sort the index as this list\n",
    "new_order = ['forward_dynamics_l1', 'forward_dynamics_l2', 'l1', 'l2', 'l1_per_action', 'l2_per_action', 'msp', 'energy']\n",
    "df = df.reindex(new_order)\n",
    "# Show them as percentages rounded to 2 decimals in string format\n",
    "df = df.map(lambda x: f'{x * 100:.2f}')\n",
    "# Change the index names and column names for the ones in the lists\n",
    "new_names = ['FwD-L1', 'FwD-L2', 'L1', 'L2', 'L1-A', 'L2-A', 'MSP', 'Energy']\n",
    "new_env_names = ['\\texttt{Lava}', '\\texttt{Ball}', '\\texttt{Sand}']\n",
    "#new_env_names = ['\\texttt{Lava}', '\\texttt{Ball}', '\\texttt{Sand}', 'Average']\n",
    "df.columns = new_env_names\n",
    "df.index = new_names\n",
    "\n",
    "# Save the dataframe as a latex table\n",
    "df.to_latex(f'ood_storage/auroc_per_setting_per_method.tex')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUROC vs Distance plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only one env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_selected = 'Lava'\n",
    "\n",
    "distances = np.arange(1, 10)\n",
    "\n",
    "# Figure settings\n",
    "figsize = (12, 6)\n",
    "fontsize = 20\n",
    "fig,ax = plt.subplots(1,1,figsize=figsize)\n",
    "\n",
    "plt.rc('text', usetex=False)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "# jdelser used plot settings\n",
    "plt.subplots_adjust(top=0.881,\n",
    "                    bottom=0.114,\n",
    "                    left=0.096,\n",
    "                    right=0.964,\n",
    "                    hspace=0.2,\n",
    "                    wspace=0.146\n",
    "                    )\n",
    "\n",
    "ax.set_facecolor('whitesmoke')\n",
    "ax.grid(True)\n",
    "ax.xaxis.offsetText.set_fontsize(fontsize)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "# Plot the AUROC values\n",
    "for method_name, values in auroc_per_env_per_method_per_distance[environment_selected].items():\n",
    "    \n",
    "    if method_name in ['forward_dynamics_l2', 'l2', 'energy']:\n",
    "\n",
    "        mean_auroc_values, std_auroc_values = values[:, 0], values[:, 1]\n",
    "        # plt.plot(distance, auroc_values, label=f'{method_name}', lw=3)\n",
    "        # x_new, bspline, y_new\n",
    "        x_new = np.linspace(1, 9, 50)\n",
    "        bspline = interpolate.make_interp_spline(distances, mean_auroc_values)\n",
    "        y_new = bspline(x_new)\n",
    "        ax.plot(x_new, y_new, label=f'{method_name}', lw=3)\n",
    "        \n",
    "        # # Normalized auroc\n",
    "        #total_auroc_value = total_auroc_per_method[method_name]\n",
    "        # plt.plot(distance, auroc_values/total_auroc_value, label=f'{method_name}', lw=3)\n",
    "\n",
    "        # FPR at a certain TPR\n",
    "        #plt.plot(distance, fpr_at_certain_tpr, label=f'{method_name}', lw=3)\n",
    "\n",
    "# Plot a dotter line in the AUROC value 1\n",
    "#plt.plot([0, 9], [1, 1], 'k--', alpha=0.5, label='Normalized AUROC = 1')\n",
    "plt.xticks(fontsize=fontsize-4)\n",
    "plt.yticks(fontsize=fontsize-4)\n",
    "# Make X axis start at 1\n",
    "plt.xlim(0.5, 9.5)\n",
    "#plt.ylim(0.6, 1)\n",
    "plt.xlabel('Distance to OOD element', fontsize=fontsize)\n",
    "plt.ylabel('AUROC', fontsize=fontsize)\n",
    "#plt.title(f'Dist to OOD vs Normalized AUROC - {environment_selected}', fontsize=fontsize+2, pad=10)\n",
    "plt.legend(fontsize=fontsize, loc='lower left')\n",
    "plt.savefig(f'ood_storage/figs/rq1/auroc_vs_dist_to_ood_{environment_selected}.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean of all envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_pickle = False\n",
    "if load_from_pickle:\n",
    "    import pickle\n",
    "    # Load auroc_per_env_per_method_per_distance from a pickle file\n",
    "    with open(f'ood_storage/auroc_per_env_per_method_per_distance.pkl', 'rb') as f:\n",
    "        auroc_per_env_per_method_per_distance = pickle.load(f)\n",
    "\n",
    "environment_selected = 'Sand050'\n",
    "\n",
    "distances = np.arange(1, 10)\n",
    "\n",
    "# Figure settings\n",
    "figsize = (11, 5)\n",
    "fontsize = 25\n",
    "fig,ax = plt.subplots(1,1,figsize=figsize)\n",
    "\n",
    "# plt.rc('text', usetex=True)\n",
    "# plt.rc('font', family='serif')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\"\n",
    "})\n",
    "\n",
    "# jdelser used plot settings\n",
    "plt.subplots_adjust(top=0.881,\n",
    "                    bottom=0.114,\n",
    "                    left=0.096,\n",
    "                    right=0.964,\n",
    "                    hspace=0.2,\n",
    "                    wspace=0.146\n",
    "                    )\n",
    "\n",
    "ax.set_facecolor('whitesmoke')\n",
    "ax.grid(True)\n",
    "ax.xaxis.offsetText.set_fontsize(fontsize)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "envs_selected = [\n",
    "    'Lava010',\n",
    "    'Ball100',\n",
    "    'Sand050',\n",
    "]\n",
    "\n",
    "mean_auroc_all_envs_per_method_per_distance = OrderedDict()\n",
    "for method_name in auroc_per_env_per_method_per_distance['Lava010'].keys():\n",
    "    auroc_one_method_all_envs = []\n",
    "    for env in envs_selected:\n",
    "        auroc_one_method_all_envs.append(auroc_per_env_per_method_per_distance[env][method_name][:, 0])\n",
    "    mean_auroc_all_envs_per_method_per_distance[method_name] = np.mean(np.array(auroc_one_method_all_envs), axis=0)    \n",
    "\n",
    "# Plot the AUROC values\n",
    "for method_name, mean_auroc_values in mean_auroc_all_envs_per_method_per_distance.items():\n",
    "    \n",
    "    if method_name in ['forward_dynamics_l1', 'l2', 'energy']:\n",
    "        \n",
    "        x_new = np.linspace(1, 9, 50)\n",
    "        bspline = interpolate.make_interp_spline(distances, mean_auroc_values)\n",
    "        y_new = bspline(x_new)\n",
    "        # Change label for the legend using a dict\n",
    "        new_labels = {\n",
    "            'forward_dynamics_l1': r'\\emph{{FwD}, $L_1$',\n",
    "            'forward_dynamics_l2': r'\\emph{FwD}, $L_2$',\n",
    "            'l2': r'\\emph{AP}, $L_2$',\n",
    "            'energy': r'\\emph{Energy}',\n",
    "        }\n",
    "        ax.plot(x_new, y_new, label=f'{new_labels[method_name]}', lw=3)\n",
    "\n",
    "# Plot a dotter line in the AUROC value 1\n",
    "#plt.plot([0, 9], [1, 1], 'k--', alpha=0.5, label='Normalized AUROC = 1')\n",
    "plt.xticks(fontsize=fontsize-4)\n",
    "plt.yticks(fontsize=fontsize-4)\n",
    "# Make X axis start at 1\n",
    "plt.xlim(0.5, 9.5)\n",
    "plt.ylim(0.45, 0.95)\n",
    "plt.xlabel('L1 Distance of the agent to the OOD element', fontsize=fontsize)\n",
    "plt.ylabel('AUROC', fontsize=fontsize)\n",
    "#plt.title(f'Dist to OOD vs Normalized AUROC - {environment_selected}', fontsize=fontsize+2, pad=10)\n",
    "plt.legend(fontsize=fontsize, loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'ood_storage/figs/rq1/auroc_vs_dist_to_ood_mean_of_envs.pdf')\n",
    "plt.show()\n",
    "\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": False,\n",
    "#     \"font.family\": \"sans-serif\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_auroc_all_envs_per_method_per_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Performance in every env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ood_storage/performance_evaluation_P_MN5S8_1000_lvls_r_ep_im005_ent00005.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_mapping = {\n",
    "    'normal_vs_lava':                           'Lava',\n",
    "    'normal_vs_ball':                           'Ball',\n",
    "    'normal_vs_sand':                           'Sand',\n",
    "    'MiniGrid-MultiRoom-N5-S8-v0':              'MultiRoomN5S8',\n",
    "    'MiniGrid-MultiRoomLava010-N5-S8-v0':       'Lava',\n",
    "    'MiniGrid-MultiRoomBall100-N5-S8-v0':       'Ball',\n",
    "    'MiniGrid-MultiRoomQuicksand050-N5-S8-v0':  'Sand',\n",
    "    'MiniGrid-MultiRoom-N5-S20-v0':             'MultiRoomN5S20',\n",
    "}\n",
    "\n",
    "env_mean_return = {\n",
    "    'MiniGrid-MultiRoom-N5-S8-v0':              [],\n",
    "    'MiniGrid-MultiRoomLava010-N5-S8-v0':      [],\n",
    "    'MiniGrid-MultiRoomBall100-N5-S8-v0':      [],\n",
    "    'MiniGrid-MultiRoomQuicksand050-N5-S8-v0':  [],\n",
    "    'MiniGrid-MultiRoom-N5-S20-v0':              [],\n",
    "\n",
    "}\n",
    "env_mean_return_in_solved_eps = deepcopy(env_mean_return)\n",
    "env_success_rate = deepcopy(env_mean_return)\n",
    "\n",
    "# Fill the lists with the repeated settings\n",
    "for i, row in df.iterrows():\n",
    "    key = row['env']\n",
    "    if key not in env_mean_return.keys():\n",
    "        continue\n",
    "    env_mean_return[key].append(row['mean_return'])\n",
    "    env_mean_return_in_solved_eps[key].append(row['mean_return_in_solved_eps'])\n",
    "    env_success_rate[key].append(row['success_rate'])\n",
    "\n",
    "# Compute the mean of all the lists\n",
    "env_mean_return = {k: np.mean(v) for k, v in env_mean_return.items()}\n",
    "env_mean_return_in_solved_eps = {k: np.mean(v) for k, v in env_mean_return_in_solved_eps.items()}\n",
    "env_success_rate = {k: np.mean(v) for k, v in env_success_rate.items()}\n",
    "\n",
    "# Create the table\n",
    "df_table = pd.DataFrame.from_dict(env_mean_return, orient='index', columns=['mean_return'])\n",
    "df_table['mean_return_in_solved_eps'] = env_mean_return_in_solved_eps.values()\n",
    "df_table['success_rate'] = np.array(list(env_success_rate.values())) * 100\n",
    "# Substite the env names with the mapping\n",
    "df_table.index = df_table.index.map(env_mapping)\n",
    "df_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only performance table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the column names using the following dict and drop the columns not included in the dict and follow the order of the dict\n",
    "new_column_names = {\n",
    "    'mean_return': 'Mean Return',\n",
    "    'success_rate': 'Success Rate [\\%]',\n",
    "    # 'energy': 'Energy',\n",
    "    # 'msp': 'MSP',\n",
    "    # 'forward_dynamics_l1': 'FD-L1',\n",
    "    # 'forward_dynamics_l2': 'FD-L2',\n",
    "    # 'l1': 'L1',\n",
    "    # 'l2': 'L2',\n",
    "    # 'l1_per_action': 'L1-A',\n",
    "    # 'l2_per_action': 'L2-A',\n",
    "}\n",
    "# Rename the columns\n",
    "df_table = df_table.rename(columns=new_column_names)\n",
    "# Drop columns that are not present in the dictionary\n",
    "columns_to_drop = [col for col in df_table.columns if col not in list(new_column_names.values())]\n",
    "df_table = df_table.drop(columns=columns_to_drop)\n",
    "# Multiply all values by 100 (except for the 'mean_return' column)\n",
    "#df_table.iloc[:, 1] *= 100\n",
    "df_table = df_table.map(lambda x: '-' if x == 0 else f\"{x:.2f}\")\n",
    "\n",
    "# Reorder the rows as in the following list\n",
    "new_index_order = [\n",
    "    'MultiRoomN5S8',\n",
    "    'MultiRoomN5S20',\n",
    "    'Lava',\n",
    "    'Ball',\n",
    "    'Sand',\n",
    "]\n",
    "# Reorder the rows\n",
    "df_table = df_table.reindex(index=new_index_order)\n",
    "\n",
    "# New index names\n",
    "new_index_names = [\n",
    "    r'\\texttt{MN5S8}',\n",
    "    r'\\texttt{MN5S20}',\n",
    "    r'\\texttt{MN5S8-Lava}',\n",
    "    r'\\texttt{MN5S8-Ball}',\n",
    "    r'\\texttt{MN5S8-Sand}',\n",
    "]\n",
    "df_table.index = new_index_names\n",
    "\n",
    "# Add a column indicating the if the environment is a OOD or Not\n",
    "df_table['OOD env'] = ['No', 'No', 'Yes', 'Yes', 'Yes']\n",
    "#df_table['OOD env'] = ['No', 'No', 'Yes','Yes','Yes', 'Yes']\n",
    "\n",
    "df_table.to_latex('ood_storage/performance_table.tex')\n",
    "df_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance + AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER STEP and for all environments\n",
    "# Define folder path\n",
    "detection_results_folder = Path(\"ood_storage/detection_results\")\n",
    "\n",
    "ood_settings_to_benchmark = [\n",
    "    'normal_vs_lava',\n",
    "    'normal_vs_ball',\n",
    "    'normal_vs_sand',\n",
    "]\n",
    "\n",
    "per_step_or_per_level_evaluation = 'per_steps'\n",
    "\n",
    "model_name = 'P_MN5S8_1000_lvls_r_ep_im005_ent00005_1'  # Para lava\n",
    "\n",
    "ood_detection_distance = 9\n",
    "\n",
    "seed = 1\n",
    "\n",
    "results_per_setting_per_method = OrderedDict()\n",
    "for ood_setting_to_benchmark in ood_settings_to_benchmark:\n",
    "    # Only consider the .pt files in the folder\n",
    "    results_per_method = OrderedDict()\n",
    "    results_per_method_per_distance = OrderedDict()\n",
    "    for file in sorted(detection_results_folder.iterdir()):\n",
    "        if file.suffix != \".pt\":\n",
    "            continue\n",
    "\n",
    "        # For each file (method), we will automatically extract the name of the method and the name of the environment from the file name\n",
    "        file_name = file.name\n",
    "        # TODO: Hack because ball and sand have different names\n",
    "        pos_start_model_name = file_name.find(model_name)\n",
    "        if pos_start_model_name == -1:  # If the model name is not in the file name, we skip the file\n",
    "            continue\n",
    "        methods_name = file_name[32:pos_start_model_name-1]\n",
    "        per_step_or_per_level_current_file = file_name[22:31]\n",
    "        ood_setting_name = file_name[file_name.find(\"ood_performance_evaluation\") + len(\"ood_performance_evaluation\") + 1 : -9]\n",
    "        ood_detection_distance_current_file = file_name[-8:-6]\n",
    "        if ood_detection_distance_current_file.isdigit():\n",
    "            ood_detection_distance_current_file = int(ood_detection_distance_current_file)\n",
    "        seed_current_file = file_name[-5:-3]\n",
    "        if seed_current_file.isdigit():\n",
    "            seed_current_file = int(file_name[-5:-3])\n",
    "        else:\n",
    "            seed_current_file = -1\n",
    "\n",
    "        if ood_setting_name != ood_setting_to_benchmark:\n",
    "            continue\n",
    "        if per_step_or_per_level_current_file != per_step_or_per_level_evaluation:\n",
    "            continue\n",
    "        if methods_name == 'performance':\n",
    "            continue\n",
    "        if ood_detection_distance_current_file != ood_detection_distance:\n",
    "            continue\n",
    "        if seed_current_file != seed:\n",
    "            continue\n",
    "        \n",
    "        #print(methods_name, '-', ood_setting_name)\n",
    "        \n",
    "        # Load the results\n",
    "        results = torch.load(file)\n",
    "\n",
    "        # Results for all distances\n",
    "        # Compute the TPR, FPR and precision to plot auroc\n",
    "        tpr_fpr_precision = extract_tpr_fpr_precision_from_results_file(results=results, per_step_results=True)\n",
    "\n",
    "        # Store the results\n",
    "        results_per_method[methods_name] = tpr_fpr_precision\n",
    "\n",
    "    # Auroc per method\n",
    "    auroc_values_per_method = compute_auroc_per_method(results_per_method=results_per_method, per_step_results=True)\n",
    "    \n",
    "    # Store the results\n",
    "    results_per_setting_per_method[ood_setting_to_benchmark] = auroc_values_per_method\n",
    "\n",
    "# Transform the hierarchy to a dict per method and per environment\n",
    "results_per_method_per_setting = OrderedDict()\n",
    "for method_name, auroc_value in auroc_values_per_method.items():\n",
    "    results_per_method_per_setting[method_name] = OrderedDict()\n",
    "    # Add the MultiRoomN5S20 with all 0s for the auroc values\n",
    "    results_per_method_per_setting[method_name]['MultiRoomN5S8'] = 0\n",
    "    for setting_name, auroc_values_per_method in results_per_setting_per_method.items():\n",
    "        results_per_method_per_setting[method_name][env_mapping[setting_name]] = results_per_setting_per_method[setting_name][method_name]\n",
    "    # Add the remaining environment (MultiRoomN5S20) with all 0s for the auroc values\n",
    "    results_per_method_per_setting[method_name]['MultiRoomN5S20'] = 0\n",
    "#results_per_method_per_setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the columns (methods) to the existing df_table. The index is the environment.\n",
    "for method_name, results_per_setting in results_per_method_per_setting.items():\n",
    "    df_table[method_name] = results_per_setting.values()\n",
    "\n",
    "# Multiply all values by 100 (except for the 'mean_return' column)\n",
    "df_table.iloc[:, 3:] *= 100\n",
    "df_table = df_table.map(lambda x: '-' if x == 0 else f\"{x:.2f}\")\n",
    "df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_table.map(lambda x: '-' if ((x == 0) and (x != df_table['mean_return'])) else f\"{x:.2f}\")\n",
    "#df_table.map(lambda x: f\"{x:.2f}\")\n",
    "#df_table.map(lambda x: '-' if x == 0 else f\"{x:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the column names using the following dict and drop the columns not included in the dict and follow the order of the dict\n",
    "new_column_names = {\n",
    "    'mean_return': 'Mean Return',\n",
    "    'success_rate': 'Success Rate',\n",
    "    'energy': 'Energy',\n",
    "    'msp': 'MSP',\n",
    "    'forward_dynamics_l1': 'FD-L1',\n",
    "    'forward_dynamics_l2': 'FD-L2',\n",
    "    'l1': 'L1',\n",
    "    'l2': 'L2',\n",
    "    'l1_per_action': 'L1-A',\n",
    "    'l2_per_action': 'L2-A',\n",
    "}\n",
    "# Rename the columns\n",
    "df_table = df_table.rename(columns=new_column_names)\n",
    "# Drop columns that are not present in the dictionary\n",
    "columns_to_drop = [col for col in df_table.columns if col not in list(new_column_names.values())]\n",
    "df_table = df_table.drop(columns=columns_to_drop)\n",
    "df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the rows as in the following list\n",
    "new_index_order = [\n",
    "    'MultiRoomN5S8',\n",
    "    'MultiRoomN5S20',\n",
    "    'Ball',\n",
    "    'Lava',\n",
    "    'Sand',\n",
    "]\n",
    "\n",
    "# Reorder the rows\n",
    "df_table = df_table.reindex(index=new_index_order)\n",
    "df_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.to_latex('ood_storage/auroc_per_method_per_env.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auroc_and_fpr_for_all_seeds(results_per_seed: List, which_tpr_for_the_fpr=0.95) -> dict:\n",
    "    fpr_at_certain_tpr = []\n",
    "    auroc_fpr_all_seeds = []\n",
    "    for i, values in enumerate(results_per_seed):\n",
    "        \n",
    "        TPR, FPR, PRECISION = values[:, 0], values[:, 1], values[:, 2]\n",
    "\n",
    "        # Compute the TPR at a certain FPR\n",
    "        index_closest = np.argmin(np.abs(TPR - which_tpr_for_the_fpr))\n",
    "        fpr_at_certain_tpr.append(FPR[index_closest])\n",
    "        auroc_fpr_all_seeds.append([np.abs(np.trapz(TPR, FPR)), FPR[index_closest]])\n",
    "\n",
    "    # Convert the lists into numpy arrays\n",
    "    auroc_fpr_all_seeds = np.array(auroc_fpr_all_seeds)\n",
    "    # Now compute the mean and std\n",
    "    auroc_mean = np.mean(auroc_fpr_all_seeds[:, 0])\n",
    "    auroc_std = np.std(auroc_fpr_all_seeds[:, 0])\n",
    "    fpr_mean = np.mean(auroc_fpr_all_seeds[:, 1])\n",
    "    fpr_std = np.std(auroc_fpr_all_seeds[:, 1])\n",
    "    \n",
    "    # Create an array with the mean and std of the auroc and fpr\n",
    "    auroc_fpr_mean_std = np.array([[auroc_mean, auroc_std], [fpr_mean, fpr_std]])\n",
    "    return auroc_fpr_mean_std\n",
    "\n",
    "#def compute_auroc_and_fpr_vs_distance_for_all_seeds()\n",
    "    \n",
    "def avg_and_std_per_distance(auroc_per_seed_per_distance: List) -> List:\n",
    "    auroc_per_distance = []\n",
    "\n",
    "    #print(auroc_per_seed_per_distance)\n",
    "    # print(len(auroc_per_seed_per_distance))\n",
    "    # print(len(auroc_per_seed_per_distance[0]))\n",
    "    # Accumulate the auroc of each seed for each distance\n",
    "    auroc_per_distance_all_seeds = OrderedDict()\n",
    "    for i in range(1, len(auroc_per_seed_per_distance[0])+1):\n",
    "        auroc_per_distance_all_seeds[i] = []\n",
    "\n",
    "    for i, values_one_seed in enumerate(auroc_per_seed_per_distance):\n",
    "        for distance, auroc_val, fpr_at_certain_tpr in values_one_seed:\n",
    "            auroc_per_distance_all_seeds[int(distance)].append(auroc_val)\n",
    "    \n",
    "    # Now compute the mean and std\n",
    "    for d, auroc_one_dist_all_seeds in auroc_per_distance_all_seeds.items():\n",
    "        #print(d, auroc_one_dist_all_seeds)\n",
    "        auroc_per_distance.append([np.mean(auroc_one_dist_all_seeds), np.std(auroc_one_dist_all_seeds)])\n",
    "    \n",
    "    return auroc_per_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
